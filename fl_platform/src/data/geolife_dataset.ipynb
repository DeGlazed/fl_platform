{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_DIR = \"raw/geolife\"\n",
    "PROCESSED_DATA_DIR = \"processed\"\n",
    "LABLES_TO_EXTRACT = ['run', 'walk', 'bus', 'car', 'taxi', 'subway', 'train', 'bike', 'motorcycle']\n",
    "raw_data = Path(RAW_DATA_DIR) / \"Geolife Trajectories 1.3\" / \"Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download dataset if not exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(RAW_DATA_DIR):\n",
    "    with tqdm(total=1) as pbar:\n",
    "        os.makedirs(RAW_DATA_DIR)\n",
    "\n",
    "        url = \"https://download.microsoft.com/download/F/4/8/F4894AA5-FDBC-481E-9285-D5F8C4C4F039/Geolife%20Trajectories%201.3.zip\"\n",
    "        with urlopen(url) as zipresp:\n",
    "            with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "                zfile.extractall(\n",
    "                    RAW_DATA_DIR\n",
    "                )\n",
    "        pbar.update()\n",
    "else :\n",
    "    print(\"Data already downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(RAW_DATA_DIR):\n",
    "    raw_data = Path(RAW_DATA_DIR) / \"Geolife Trajectories 1.3\" / \"Data\"\n",
    "\n",
    "    for participant_folder in raw_data.iterdir():\n",
    "        if participant_folder.is_dir():\n",
    "            contents = list(participant_folder.iterdir())\n",
    "            if len(contents) != 2 or not any(item.is_file() for item in contents) or not any(item.is_dir() for item in contents):\n",
    "                print(f\"Removing folder: {participant_folder}\")\n",
    "                shutil.rmtree(participant_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(RAW_DATA_DIR):\n",
    "    raw_data = Path(RAW_DATA_DIR) / \"Geolife Trajectories 1.3\" / \"Data\"\n",
    "\n",
    "    participants = sorted(raw_data.iterdir())\n",
    "    for idx, participant_folder in enumerate(participants, start=1):\n",
    "        new_name = raw_data / f\"{idx}\"\n",
    "        participant_folder.rename(new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi threaded version\n",
    "import threading\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "def process_trajectory_file(file_path):\n",
    "    df = pd.read_csv(file_path, skiprows=6, header=None, usecols=[0, 1, 5, 6], names=['latitude', 'longitude', 'date', 'time'])\n",
    "    df['date_time'] = pd.to_datetime(df['date'] + ' ' + df['time'])\n",
    "    df.drop(columns=['date', 'time'], inplace=True)\n",
    "    df['timestamp'] = df['date_time'].astype(np.int64) / 10**9\n",
    "    return df\n",
    "\n",
    "PROCESSED_DATA = {}\n",
    "\n",
    "def process_participant_folder(participant_folder):\n",
    "    print(f\"Processing folder: {participant_folder.name}\")\n",
    "    \n",
    "    if participant_folder.is_dir():\n",
    "        labels_file = participant_folder / \"labels.txt\"\n",
    "        trajectory_folder = participant_folder / \"Trajectory\"\n",
    "        \n",
    "        if labels_file.exists() and trajectory_folder.exists():\n",
    "            print(f\"Processing {participant_folder.name} labels\")\n",
    "            # Read the labels file and create a dataframe\n",
    "            labels_df = pd.read_csv(labels_file, sep='\\t')\n",
    "\n",
    "            labels_df['Start Time'] = pd.to_datetime(labels_df['Start Time'])\n",
    "            labels_df['End Time'] = pd.to_datetime(labels_df['End Time'])\n",
    "            labels_df['Transportation Mode'] = labels_df['Transportation Mode'].astype(str)\n",
    "\n",
    "            labels_df = labels_df[labels_df['Transportation Mode'].isin(LABLES_TO_EXTRACT)].reset_index(drop=True)\n",
    "\n",
    "            # Get the trajectory files\n",
    "            trajectory_files = sorted(list(trajectory_folder.glob(\"*.plt\")))\n",
    "            trajectory_dataframes = [process_trajectory_file(file) for file in trajectory_files]\n",
    "\n",
    "            print(f\"Checking {participant_folder.name} labels for null values\")\n",
    "            # Check for null values and empty dataframes\n",
    "            for i, df in enumerate(trajectory_dataframes):\n",
    "                if df.isnull().values.any():\n",
    "                    print(f\"DataFrame at index {i} contains null values.\")\n",
    "                if df.empty:\n",
    "                    print(f\"DataFrame at index {i} is empty.\")\n",
    "\n",
    "            print(f\"Extracting {participant_folder.name} data\")\n",
    "            # Extract the dataframes for each row in the labels dataframe\n",
    "            extracted_dataframes = {}\n",
    "\n",
    "            for index, row in labels_df.iterrows():\n",
    "                start_time = row['Start Time']\n",
    "                end_time = row['End Time']\n",
    "                label = row['Transportation Mode']\n",
    "\n",
    "                filtered_dfs = []\n",
    "                for df in trajectory_dataframes:\n",
    "                    filtered_df = df[(df['date_time'] >= start_time) & (df['date_time'] <= end_time)]\n",
    "                    if not filtered_df.empty:\n",
    "                        filtered_dfs.append(filtered_df)\n",
    "\n",
    "                if filtered_dfs:\n",
    "                    combined_df = pd.concat(filtered_dfs)\n",
    "                    extracted_dataframes[f\"{index}_{label}\"] = combined_df\n",
    "\n",
    "            print(f\"Checking {participant_folder.name} data for null values\")\n",
    "            # Check for null values and empty dataframes\n",
    "            for i, df in extracted_dataframes.items():\n",
    "                if df.isnull().values.any():\n",
    "                    print(f\"DataFrame at index {i} contains null values.\")\n",
    "                if df.empty:\n",
    "                    print(f\"DataFrame at index {i} is empty.\")\n",
    "            \n",
    "            with lock:\n",
    "                print(f\"Adding {participant_folder.name} data to final result\")\n",
    "                PROCESSED_DATA[participant_folder.name] = extracted_dataframes\n",
    "                print(f\"Done adding {participant_folder.name} data to final result\")\n",
    "\n",
    "# Use ThreadPoolExecutor to process participant folders in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:\n",
    "    executor.map(process_participant_folder, raw_data.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants_with_no_data = [participant for participant, dataframes in PROCESSED_DATA.items() if not dataframes]\n",
    "print(\"Participants with no data:\", participants_with_no_data)\n",
    "\n",
    "# Remove participants with no data from PROCESSED_DATA\n",
    "PROCESSED_DATA = {participant: dataframes for participant, dataframes in PROCESSED_DATA.items() if dataframes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for participant, dataframes in PROCESSED_DATA.items():\n",
    "    for label, df in dataframes.items():\n",
    "        dataframes[label] = df.sort_values(by='date_time').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tmp_processed_data = {}\n",
    "\n",
    "for new_id, (participant, dataframes) in enumerate(PROCESSED_DATA.items(), start=1):\n",
    "    tmp_processed_data[new_id] = dataframes\n",
    "\n",
    "PROCESSED_DATA = tmp_processed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if not os.path.exists(PROCESSED_DATA_DIR):\n",
    "    os.makedirs(PROCESSED_DATA_DIR)\n",
    "    \n",
    "with open(os.path.join(PROCESSED_DATA_DIR, 'geolife_processed_data.pkl'), 'wb') as f:\n",
    "    pickle.dump(PROCESSED_DATA, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from the processed data file\n",
    "import pickle\n",
    "\n",
    "LOADED_DATA = {}\n",
    "\n",
    "# Load the processed data\n",
    "with open(os.path.join(PROCESSED_DATA_DIR, 'geolife_processed_data.pkl'), 'rb') as f:\n",
    "    LOADED_DATA = pickle.load(f)\n",
    "\n",
    "# Test the dataframes\n",
    "for participant, dataframes in LOADED_DATA.items():\n",
    "    for label, df in dataframes.items():\n",
    "        print(f\"Participant: {participant}, Label: {label}\")\n",
    "        print(df.info())\n",
    "        print(df.head())\n",
    "        break  # Remove this break to print all dataframes\n",
    "    break  # Remove this break to print all participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "participant_dataframes = LOADED_DATA[60]\n",
    "\n",
    "# Create a map centered around Beijing\n",
    "beijing_map = folium.Map(location=[39.9042, 116.4074], zoom_start=12)\n",
    "\n",
    "# Plot each route for participant 1\n",
    "for label, df in participant_dataframes.items():\n",
    "    route = folium.PolyLine(\n",
    "        locations=df[['latitude', 'longitude']].values,\n",
    "        color='blue',\n",
    "        weight=2.5,\n",
    "        opacity=1\n",
    "    )\n",
    "    route.add_to(beijing_map)\n",
    "\n",
    "# Display the map\n",
    "beijing_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(LOADED_DATA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a dictionary to store the number of entries for each participant\n",
    "participant_entries = {participant: len(dataframes) for participant, dataframes in LOADED_DATA.items()}\n",
    "\n",
    "# Plot the number of entries for each participant\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(participant_entries.keys(), participant_entries.values())\n",
    "plt.xlabel('Participant')\n",
    "plt.ylabel('Number of Routes')\n",
    "plt.title('Number of Routes for Each Participant')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "label_distribution = Counter()\n",
    "\n",
    "for participant, dataframes in LOADED_DATA.items():\n",
    "    for label in dataframes.keys():\n",
    "        label_distribution[label.split('_')[1]] += 1\n",
    "\n",
    "print(label_distribution)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(label_distribution.keys(), label_distribution.values())\n",
    "plt.xlabel('Transportation Mode')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Transportation Modes')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# due to the low count of run and motorcycle data, i would remove them\n",
    "#or retag them as walk and car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "print(type(LOADED_DATA[1]['0_bus']))\n",
    "pprint.pprint(LOADED_DATA[1]['0_bus'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
